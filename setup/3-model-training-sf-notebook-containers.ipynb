{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e0173376-9eb5-4a7e-ac3b-b66ea65fded6",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "!pip install snowflake-ml-python --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ecfee14",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "# MONAI Lung CT Registration - Model Training\n",
        "\n",
        "This notebook trains a **LocalNet** deep learning model for 3D CT lung registration using the MONAI framework.\n",
        "\n",
        "## Architecture Overview\n",
        "- **Model**: LocalNet (spatial_dims=3, in_channels=2, out_channels=3)\n",
        "- **Loss**: Multi-scale local NCC (Normalized Cross-Correlation)\n",
        "- **Regularization**: Bending energy on deformation field\n",
        "- **Framework**: MONAI + PyTorch\n",
        "\n",
        "## Model Registration\n",
        "After training, the model is registered to **Snowflake Model Registry** using a `CustomModel` class that supports `run_batch()` for distributed GPU inference on compute pools."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe347f4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 1: Initialize Snowflake Session\n",
        "\n",
        "Import core libraries and establish connection to Snowflake's active session. This session provides access to:\n",
        "- Snowflake stages for data storage\n",
        "- SQL execution capabilities\n",
        "- File I/O operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "11786677-8f2c-4e69-9bf1-89ad993cd03b",
      "metadata": {
        "language": "python",
        "name": "cell1"
      },
      "outputs": [],
      "source": [
        "# Core Python libraries for data manipulation and UI\n",
        "import pandas as pd     # For tabular data handling\n",
        "# Snowflake-specific imports\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "# Establish connection to the active Snowflake session\n",
        "# This provides access to stages, warehouses, and compute resources\n",
        "session = get_active_session()\n",
        "session.use_database('SF_CLINICAL_DB')\n",
        "session.use_schema('UTILS')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcbf7ac8",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 2: Configure Snowflake ML Jobs\n",
        "\n",
        "Snowflake ML Jobs provide a simpler alternative to Ray for distributed GPU training:\n",
        "\n",
        "- **No cluster management**: Jobs automatically provision GPU resources\n",
        "- **Automatic dependency handling**: Container Runtime includes ML libraries\n",
        "- **Native Snowflake integration**: Direct access to stages and tables\n",
        "\n",
        "The `@remote` decorator submits functions to run on GPU compute pools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83815555-4313-4b34-8ef3-9f95dabf2394",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SNOWFLAKE ML JOBS SETUP\n",
        "# ============================================================================\n",
        "# Using Snowflake ML Jobs instead of Ray for distributed GPU training.\n",
        "# Jobs automatically handle compute pool allocation and dependency management.\n",
        "\n",
        "# Standard Python libraries\n",
        "import pandas as pd\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Deep learning frameworks\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Snowflake integrations\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.files import SnowflakeFile\n",
        "from snowflake.ml.jobs import remote  # ML Jobs API\n",
        "\n",
        "session = get_active_session()\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Compute pool for GPU training jobs\n",
        "TRAINING_COMPUTE_POOL = \"GPU_ML_M_POOL\"  # Must have GPU instances (e.g., GPU_NV_S)\n",
        "PAYLOAD_STAGE = \"@SF_CLINICAL_DB.UTILS.RESULTS_STG/job_payloads\"\n",
        "\n",
        "print(\"âœ… Snowflake ML Jobs configured\")\n",
        "print(f\"   Compute Pool: {TRAINING_COMPUTE_POOL}\")\n",
        "print(f\"   Payload Stage: {PAYLOAD_STAGE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1834a4cb-1bb4-4867-b5c0-f8ba6279bc6e",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "!pip install nibabel monai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c7db73",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 3: Data Exploration - Visualize Medical Images\n",
        "\n",
        "Before training, let's inspect our data! This cell provides an interactive viewer for NIfTI (Neuroimaging Informatics Technology Initiative) files stored in Snowflake stages.\n",
        "\n",
        "**NIfTI Format**: Standard medical imaging format storing 3D volumetric data (like CT or MRI scans).\n",
        "\n",
        "**Interactive Slider**: Navigate through the scan's depth (z-axis) to view different slices of the lung."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "78827d26-d75e-4951-9ccd-953c677c0620",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "from snowflake.snowpark.files import SnowflakeFile\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "from monai.networks.nets import LocalNet\n",
        "from monai.networks.blocks import Warp\n",
        "from monai.losses import GlobalMutualInformationLoss, BendingEnergyLoss\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityd, Resized\n",
        ")\n",
        "from monai.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "def load_nifti_from_stage(stage_path):\n",
        "    session = get_active_session()\n",
        "    clean_path = stage_path if stage_path.startswith(\"@\") else f\"@{stage_path}\"\n",
        "    \n",
        "    try:\n",
        "        with SnowflakeFile.open(clean_path, 'rb') as f:\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".nii.gz\", delete=False) as tmp:\n",
        "                tmp.write(f.read())\n",
        "                tmp_name = tmp.name\n",
        "        \n",
        "        img = nib.load(tmp_name)\n",
        "        data = img.get_fdata()\n",
        "        os.unlink(tmp_name)\n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {clean_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def visualize_nifti_slices(stage_path, num_slices=5):\n",
        "    print(f\"Viewing: {stage_path}\")\n",
        "    \n",
        "    vol_data = load_nifti_from_stage(stage_path)\n",
        "    if vol_data is None:\n",
        "        return\n",
        "\n",
        "    x, y, z = vol_data.shape\n",
        "    print(f\"Volume Shape: {vol_data.shape}\")\n",
        "\n",
        "    slice_indices = [int(i * (z - 1) / (num_slices - 1)) for i in range(num_slices)]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, num_slices, figsize=(4 * num_slices, 4))\n",
        "    \n",
        "    for ax, idx in zip(axes, slice_indices):\n",
        "        slice_data = np.rot90(vol_data[:, :, idx])\n",
        "        ax.imshow(slice_data, cmap='gray')\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"Slice {idx}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    display(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def visualize_nifti_slice(stage_path, slice_idx=None):\n",
        "    vol_data = load_nifti_from_stage(stage_path)\n",
        "    if vol_data is None:\n",
        "        return\n",
        "\n",
        "    x, y, z = vol_data.shape\n",
        "    if slice_idx is None:\n",
        "        slice_idx = z // 2\n",
        "    \n",
        "    slice_data = np.rot90(vol_data[:, :, slice_idx])\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.imshow(slice_data, cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Slice {slice_idx} of {z}\")\n",
        "    display(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "sample_path = \"sf_clinical_db.utils.monai_medical_images_stg/scansExp/case_001_exp.nii.gz\"\n",
        "visualize_nifti_slices(sample_path, num_slices=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d540ab",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 4: Load Paired Image Paths (Metadata Only)\n",
        "\n",
        "**Key Insight**: We DON'T load the actual image data yet - just the file paths!\n",
        "\n",
        "This cell:\n",
        "1. Lists all NIfTI files in the Snowflake stage using SQL\n",
        "2. Pairs \"fixed\" (expiration) and \"moving\" (inspiration) scans\n",
        "3. Returns a list of dictionaries containing file paths\n",
        "\n",
        "**Why paths only?** Loading all medical images into memory would consume GB/TB of RAM. Instead, we'll load images just-in-time during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d4ef797-5c60-4ac6-9b32-76ac85664896",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": [
        "def load_paired_paths(stage_location=\"@SF_CLINICAL_DB.UTILS.MONAI_MEDICAL_IMAGES_STG\"):\n",
        "    \"\"\"\n",
        "    Load paired CT scan paths for medical image registration.\n",
        "    \n",
        "    This function performs METADATA OPERATIONS ONLY - no actual image data is loaded.\n",
        "    It identifies pairs of:\n",
        "    - Fixed image (expiration CT)\n",
        "    - Moving image (inspiration CT)\n",
        "    - Fixed label (lung mask for expiration)\n",
        "    - Moving label (lung mask for inspiration)\n",
        "    \n",
        "    Args:\n",
        "        stage_location (str): Snowflake stage containing medical images\n",
        "    \n",
        "    Returns:\n",
        "        list: List of dictionaries with file paths for each case\n",
        "    \"\"\"\n",
        "    session = get_active_session()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: List all NIfTI files using SQL (fast, metadata-only operation)\n",
        "    # ========================================================================\n",
        "    df_files = session.sql(\n",
        "        f\"LIST {stage_location} PATTERN = '.*.nii.gz'\"\n",
        "    ).select('\"name\"').to_pandas()\n",
        "    \n",
        "    df_files.rename(columns={'\"name\"': 'name'})\n",
        "    df_files[\"name\"] = \"UTILS.\" + df_files[\"name\"]\n",
        "  \n",
        "    print(df_files)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: Categorize files by type using regex patterns\n",
        "    # ========================================================================\n",
        "    fixed_imgs = df_files[df_files['name'].str.contains(\"scans.*_exp\", regex=True)]\n",
        "    moving_imgs = df_files[df_files['name'].str.contains(\"scans.*_insp\", regex=True)]\n",
        "    fixed_masks = df_files[df_files['name'].str.contains(\"lungMasks.*_exp\", regex=True)]\n",
        "    moving_masks = df_files[df_files['name'].str.contains(\"lungMasks.*_insp\", regex=True)]\n",
        "    \n",
        "    pairs = []\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 3: Match files into training pairs using case IDs\n",
        "    # ========================================================================\n",
        "    for _, row in fixed_imgs.iterrows():\n",
        "        f_path = row['name']\n",
        "        filename = f_path.split('/')[-1] \n",
        "        case_id = filename.split('_exp')[0] \n",
        "        \n",
        "        m_path = moving_imgs[moving_imgs['name'].str.contains(f\"{case_id}_insp\")].iloc[0]['name']\n",
        "        fl_path = fixed_masks[fixed_masks['name'].str.contains(f\"{case_id}_exp\")].iloc[0]['name']\n",
        "        ml_path = moving_masks[moving_masks['name'].str.contains(f\"{case_id}_insp\")].iloc[0]['name']\n",
        "        \n",
        "        pairs.append({\n",
        "            \"case_id\": case_id,\n",
        "            \"fixed_path\": f_path,\n",
        "            \"moving_path\": m_path,\n",
        "            \"fixed_label_path\": fl_path,\n",
        "            \"moving_label_path\": ml_path\n",
        "        })\n",
        "        \n",
        "    print(f\"âœ… Paired {len(pairs)} cases (paths only, no binary data loaded)\")\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTE: Load paired file paths\n",
        "# ============================================================================\n",
        "paired_data = load_paired_paths()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1952285",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 5: Define Custom Dataset with Just-In-Time Loading\n",
        "\n",
        "This cell creates a PyTorch `Dataset` that:\n",
        "1. **Loads images on-demand** - Downloads from Snowflake stage only when needed\n",
        "2. **Applies preprocessing** - Normalizes intensities, resizes volumes\n",
        "3. **Handles labels correctly** - Preserves binary masks without intensity scaling\n",
        "\n",
        "**Why custom dataset?** Standard datasets can't handle Snowflake stage I/O. This custom implementation downloads each file to a temporary location, processes it, then cleans up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7729555b-b088-42bd-9ee4-4e73c061c074",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell5"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# HELPER FUNCTIONS FOR SNOWFLAKE STAGE I/O\n",
        "# ============================================================================\n",
        "\n",
        "def read_file_from_stage(stage_path):\n",
        "    \"\"\"\n",
        "    Read binary file content from Snowflake stage.\n",
        "    \n",
        "    Args:\n",
        "        stage_path (str): Path to file in stage (with or without @ prefix)\n",
        "    \n",
        "    Returns:\n",
        "        bytes: Raw file content\n",
        "    \"\"\"\n",
        "    session = get_active_session()\n",
        "    # Ensure path has @ prefix for Snowflake stage access\n",
        "    clean_path = stage_path if stage_path.startswith(\"@\") else f\"@{stage_path}\"\n",
        "    \n",
        "    with SnowflakeFile.open(clean_path, 'rb') as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CUSTOM PYTORCH DATASET WITH SNOWFLAKE INTEGRATION\n",
        "# ============================================================================\n",
        "\n",
        "class SnowflakeStageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for medical images stored in Snowflake stages.\n",
        "    \n",
        "    Key Features:\n",
        "    1. **Just-In-Time Loading**: Downloads images from Snowflake only when needed\n",
        "    2. **Automatic Cleanup**: Deletes temporary files after processing\n",
        "    3. **MONAI Integration**: Applies medical imaging transformations\n",
        "    4. **Separate Transforms**: Different pipelines for images vs. labels\n",
        "    \n",
        "    This approach avoids loading all GB/TB of medical data into memory at once.\n",
        "    \n",
        "    Args:\n",
        "        data_dicts (list): List of dicts with keys like 'fixed_path', 'moving_path', etc.\n",
        "        transform_img (Callable): MONAI transforms for images (includes normalization)\n",
        "        transform_lbl (Callable): MONAI transforms for labels (NO normalization)\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dicts, transform_img=None, transform_lbl=None):\n",
        "        self.data_dicts = data_dicts\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_lbl = transform_lbl\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return number of image pairs in dataset.\"\"\"\n",
        "        return len(self.data_dicts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Load and preprocess a single training sample.\n",
        "        \n",
        "        This method:\n",
        "        1. Downloads 4 files from Snowflake (fixed/moving images + labels)\n",
        "        2. Saves them to temporary locations\n",
        "        3. Applies MONAI transformations\n",
        "        4. Cleans up temporary files\n",
        "        5. Returns preprocessed tensors\n",
        "        \n",
        "        Args:\n",
        "            index (int): Dataset index\n",
        "        \n",
        "        Returns:\n",
        "            dict: Dictionary with 'fixed', 'moving', 'fixed_label', 'moving_label' tensors\n",
        "        \"\"\"\n",
        "        item = self.data_dicts[index]\n",
        "        \n",
        "        # ====================================================================\n",
        "        # STEP 1: Download files from Snowflake to temporary locations\n",
        "        # ====================================================================\n",
        "        temp_files = {}\n",
        "        for key in [\"fixed\", \"moving\", \"fixed_label\", \"moving_label\"]:\n",
        "            path = item[f\"{key}_path\"]\n",
        "            binary = read_file_from_stage(path)\n",
        "            \n",
        "            # Create temporary file with .nii.gz extension (required by nibabel)\n",
        "            tf = tempfile.NamedTemporaryFile(suffix=\".nii.gz\", delete=False)\n",
        "            tf.write(binary)\n",
        "            tf.close()\n",
        "            temp_files[key] = tf.name\n",
        "        \n",
        "        # ====================================================================\n",
        "        # STEP 2: Prepare data dictionary for MONAI transforms\n",
        "        # ====================================================================\n",
        "        # MONAI expects file paths as input to LoadImaged transform\n",
        "        data = {\n",
        "            \"fixed\": temp_files[\"fixed\"],\n",
        "            \"moving\": temp_files[\"moving\"],\n",
        "            \"fixed_label\": temp_files[\"fixed_label\"],\n",
        "            \"moving_label\": temp_files[\"moving_label\"],\n",
        "        }\n",
        "        \n",
        "        # ====================================================================\n",
        "        # STEP 3: Apply MONAI transformations (load, normalize, resize, augment)\n",
        "        # ====================================================================\n",
        "        if self.transform_img:\n",
        "            data = self.transform_img(data)\n",
        "        \n",
        "        # ====================================================================\n",
        "        # STEP 4: Clean up temporary files to free disk space\n",
        "        # ====================================================================\n",
        "        for fpath in temp_files.values():\n",
        "            if os.path.exists(fpath):\n",
        "                os.unlink(fpath)\n",
        "        \n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb6a6c0",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 6: Define Training Function\n",
        "\n",
        "This is the core training loop submitted as a **Snowflake ML Job**:\n",
        "\n",
        "1. **Defines Transforms** - Preprocessing pipeline for medical images\n",
        "2. **Creates Data Loaders** - Wraps our custom dataset for batch iteration\n",
        "3. **Builds Model** - LocalNet architecture for deformation field prediction\n",
        "4. **Training Loop** - Iterates through epochs, computing loss and updating weights\n",
        "5. **Validation** - Monitors Dice score on held-out data\n",
        "6. **Model Checkpointing** - Saves best model to Snowflake stage\n",
        "\n",
        "The `@remote` decorator submits this function to run on a GPU compute pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8d955bfc-126d-456c-8445-a1a78e923e7e",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell6"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MONAI MEDICAL IMAGING IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "from monai.networks.nets import LocalNet\n",
        "from monai.networks.blocks import Warp\n",
        "from monai.losses import GlobalMutualInformationLoss, BendingEnergyLoss\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    ScaleIntensityRanged,\n",
        "    Resized,\n",
        "    LoadImage,\n",
        "    EnsureChannelFirst,\n",
        "    ScaleIntensityRange,\n",
        "    Resize,\n",
        "    RandAffined,\n",
        "    RandGaussianNoised,\n",
        "    RandGaussianSmoothd\n",
        ")\n",
        "from monai.data import Dataset, DataLoader\n",
        "\n",
        "CT_MIN_HU = -1000\n",
        "CT_MAX_HU = 500\n",
        "\n",
        "CONFIG = {\n",
        "    \"spatial_size\": (96, 96, 104),\n",
        "    \"num_channel_initial\": 32,\n",
        "    \"batch_size\": 2,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"max_epochs\": 25,\n",
        "    \"reg_weight\": 1.0,\n",
        "    \"save_interval\": 10,\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DISTRIBUTED TRAINING FUNCTION (GPU-ACCELERATED)\n",
        "# ============================================================================\n",
        "\n",
        "@remote(\n",
        "    compute_pool=TRAINING_COMPUTE_POOL,\n",
        "    stage_name=PAYLOAD_STAGE,\n",
        "    pip_requirements=[\"monai\", \"nibabel\", \"pytorch-ignite\"],\n",
        "    external_access_integrations=[\"ALLOW_ALL_EAI\"]\n",
        ")\n",
        "def train_registration_model(\n",
        "    train_files, \n",
        "    val_files=None,\n",
        "    config=None,\n",
        "    save_path=\"@SF_CLINICAL_DB.UTILS.RESULTS_STG\"\n",
        "):\n",
        "    import torch\n",
        "    import torch.optim as optim\n",
        "    import numpy as np\n",
        "    import tempfile\n",
        "    import os\n",
        "    \n",
        "    from snowflake.snowpark.context import get_active_session\n",
        "    from snowflake.snowpark.files import SnowflakeFile\n",
        "    from monai.networks.nets import LocalNet\n",
        "    from monai.networks.blocks import Warp\n",
        "    from monai.losses import GlobalMutualInformationLoss, BendingEnergyLoss\n",
        "    from monai.transforms import (\n",
        "        Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityRanged, Resized,\n",
        "        RandAffined, RandGaussianNoised, RandGaussianSmoothd\n",
        "    )\n",
        "    from monai.data import Dataset as MonaiDataset, DataLoader\n",
        "    \n",
        "    CT_MIN_HU = -1000\n",
        "    CT_MAX_HU = 500\n",
        "    \n",
        "    if config is None:\n",
        "        config = {\n",
        "            \"spatial_size\": (96, 96, 104),\n",
        "            \"num_channel_initial\": 32,\n",
        "            \"batch_size\": 2,\n",
        "            \"learning_rate\": 1e-4,\n",
        "            \"max_epochs\": 25,\n",
        "            \"reg_weight\": 1.0,\n",
        "            \"save_interval\": 10,\n",
        "        }\n",
        "    \n",
        "    print(f\"ðŸš€ Starting training with config: {config}\")\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"ðŸ“ Using device: {device}\")\n",
        "    \n",
        "    def read_file_from_stage(stage_path):\n",
        "        clean_path = stage_path if stage_path.startswith(\"@\") else f\"@{stage_path}\"\n",
        "        with SnowflakeFile.open(clean_path, 'rb') as f:\n",
        "            return f.read()\n",
        "    \n",
        "    class SnowflakeStageDataset(MonaiDataset):\n",
        "        def __init__(self, data_dicts, transform_img=None):\n",
        "            self.data_dicts = data_dicts\n",
        "            self.transform_img = transform_img\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data_dicts)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            item = self.data_dicts[index]\n",
        "            temp_files = {}\n",
        "            for key in [\"fixed\", \"moving\", \"fixed_label\", \"moving_label\"]:\n",
        "                path = item[f\"{key}_path\"]\n",
        "                binary = read_file_from_stage(path)\n",
        "                tf = tempfile.NamedTemporaryFile(suffix=\".nii.gz\", delete=False)\n",
        "                tf.write(binary)\n",
        "                tf.close()\n",
        "                temp_files[key] = tf.name\n",
        "            \n",
        "            data = {\n",
        "                \"fixed\": temp_files[\"fixed\"],\n",
        "                \"moving\": temp_files[\"moving\"],\n",
        "                \"fixed_label\": temp_files[\"fixed_label\"],\n",
        "                \"moving_label\": temp_files[\"moving_label\"],\n",
        "            }\n",
        "            \n",
        "            if self.transform_img:\n",
        "                data = self.transform_img(data)\n",
        "            \n",
        "            for fpath in temp_files.values():\n",
        "                if os.path.exists(fpath):\n",
        "                    os.unlink(fpath)\n",
        "            \n",
        "            return data\n",
        "    \n",
        "    train_img_transforms = Compose([\n",
        "        LoadImaged(keys=[\"fixed\", \"moving\", \"fixed_label\", \"moving_label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"fixed\", \"moving\", \"fixed_label\", \"moving_label\"]),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"fixed\", \"moving\"],\n",
        "            a_min=CT_MIN_HU,\n",
        "            a_max=CT_MAX_HU,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True\n",
        "        ),\n",
        "        Resized(\n",
        "            keys=[\"fixed\", \"moving\"], \n",
        "            spatial_size=config[\"spatial_size\"],\n",
        "            mode=\"trilinear\"\n",
        "        ),\n",
        "        Resized(\n",
        "            keys=[\"fixed_label\", \"moving_label\"], \n",
        "            spatial_size=config[\"spatial_size\"], \n",
        "            mode=\"nearest\"\n",
        "        ),\n",
        "    ])\n",
        "    \n",
        "    val_transforms = train_img_transforms\n",
        "    \n",
        "    train_ds = SnowflakeStageDataset(train_files, transform_img=train_img_transforms)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, \n",
        "        batch_size=config[\"batch_size\"], \n",
        "        shuffle=True, \n",
        "        num_workers=0\n",
        "    )\n",
        "    \n",
        "    val_loader = None\n",
        "    if val_files:\n",
        "        val_ds = SnowflakeStageDataset(val_files, transform_img=val_transforms)\n",
        "        val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0)\n",
        "    \n",
        "    model = LocalNet(\n",
        "        spatial_dims=3,\n",
        "        in_channels=2,\n",
        "        out_channels=3,\n",
        "        num_channel_initial=config[\"num_channel_initial\"],\n",
        "        extract_levels=[3],\n",
        "        out_activation=None,\n",
        "        out_kernel_initializer=\"zeros\"\n",
        "    ).to(device)\n",
        "    \n",
        "    warp_layer = Warp().to(device)\n",
        "    \n",
        "    loss_sim = GlobalMutualInformationLoss()\n",
        "    loss_reg = BendingEnergyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "    \n",
        "    best_val_dice = 0.0\n",
        "    training_history = []\n",
        "    \n",
        "    def save_model_to_stage(model, stage_path):\n",
        "        session = get_active_session()\n",
        "        filename = stage_path.split('/')[-1]\n",
        "        local_path = f\"/tmp/{filename}\"\n",
        "        torch.save(model.state_dict(), local_path)\n",
        "        stage_dir = '/'.join(stage_path.split('/')[:-1])\n",
        "        try:\n",
        "            session.file.put(local_path, stage_dir, overwrite=True, auto_compress=False)\n",
        "            print(f\"âœ… Saved to {stage_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to save: {e}\")\n",
        "        finally:\n",
        "            if os.path.exists(local_path):\n",
        "                os.unlink(local_path)\n",
        "    \n",
        "    for epoch in range(config[\"max_epochs\"]):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sim_loss = 0\n",
        "        epoch_reg_loss = 0\n",
        "        step = 0\n",
        "        \n",
        "        for batch_data in train_loader:\n",
        "            step += 1\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            fixed = batch_data[\"fixed\"].to(device)\n",
        "            moving = batch_data[\"moving\"].to(device)\n",
        "            \n",
        "            input_tensor = torch.cat((moving, fixed), dim=1)\n",
        "            ddf = model(input_tensor)\n",
        "            pred_image = warp_layer(moving, ddf)\n",
        "            \n",
        "            sim_loss = loss_sim(pred_image, fixed)\n",
        "            reg_loss = loss_reg(ddf)\n",
        "            total_loss = sim_loss + config[\"reg_weight\"] * reg_loss\n",
        "            \n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += total_loss.item()\n",
        "            epoch_sim_loss += sim_loss.item()\n",
        "            epoch_reg_loss += reg_loss.item()\n",
        "        \n",
        "        avg_loss = epoch_loss / step\n",
        "        avg_sim = epoch_sim_loss / step\n",
        "        avg_reg = epoch_reg_loss / step\n",
        "        \n",
        "        print(f\"âœ… Epoch {epoch + 1}/{config['max_epochs']}\")\n",
        "        print(f\"   Loss: {avg_loss:.4f} (Sim: {avg_sim:.4f}, Reg: {avg_reg:.4f})\")\n",
        "        \n",
        "        avg_val_dice = None\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_dice_scores = []\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for val_batch in val_loader:\n",
        "                    fixed = val_batch[\"fixed\"].to(device)\n",
        "                    moving = val_batch[\"moving\"].to(device)\n",
        "                    fixed_lbl = val_batch[\"fixed_label\"].to(device)\n",
        "                    moving_lbl = val_batch[\"moving_label\"].to(device)\n",
        "                    \n",
        "                    input_tensor = torch.cat((moving, fixed), dim=1)\n",
        "                    ddf = model(input_tensor)\n",
        "                    pred_label = warp_layer(moving_lbl, ddf)\n",
        "                    \n",
        "                    intersection = (pred_label * fixed_lbl).sum()\n",
        "                    dice = (2.0 * intersection + 1e-5) / (pred_label.sum() + fixed_lbl.sum() + 1e-5)\n",
        "                    val_dice_scores.append(dice.item())\n",
        "            \n",
        "            avg_val_dice = np.mean(val_dice_scores)\n",
        "            print(f\"   Val Dice: {avg_val_dice:.4f}\")\n",
        "            \n",
        "            scheduler.step(avg_loss)\n",
        "            \n",
        "            if avg_val_dice > best_val_dice:\n",
        "                best_val_dice = avg_val_dice\n",
        "                print(f\"   ðŸ† New best model! Dice: {best_val_dice:.4f}\")\n",
        "                save_model_to_stage(model, f\"{save_path}/best_model.pth\")\n",
        "        \n",
        "        training_history.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_loss,\n",
        "            \"train_sim\": avg_sim,\n",
        "            \"train_reg\": avg_reg,\n",
        "            \"val_dice\": avg_val_dice\n",
        "        })\n",
        "        \n",
        "        if (epoch + 1) % config[\"save_interval\"] == 0:\n",
        "            checkpoint_path = f\"{save_path}/checkpoint_epoch_{epoch+1}.pth\"\n",
        "            save_model_to_stage(model, checkpoint_path)\n",
        "            print(f\"   ðŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
        "    \n",
        "    final_path = f\"{save_path}/final_model.pth\"\n",
        "    save_model_to_stage(model, final_path)\n",
        "    print(f\"ðŸ Training complete! Final model: {final_path}\")\n",
        "    \n",
        "    return {\n",
        "        \"final_model_path\": final_path,\n",
        "        \"best_model_path\": f\"{save_path}/best_model.pth\",\n",
        "        \"best_val_dice\": best_val_dice,\n",
        "        \"training_history\": training_history\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9b3abc",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 7: Prepare Training and Validation Sets\n",
        "\n",
        "Now we split our dataset into:\n",
        "- **Training Set (80%)** - Used for model optimization\n",
        "- **Validation Set (20%)** - Used for monitoring performance and early stopping\n",
        "\n",
        "This split helps prevent overfitting and ensures our model generalizes to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9819b251-0382-43de-8d7a-b39a2545c1fe",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell7"
      },
      "outputs": [],
      "source": [
        "train_files_list = []\n",
        "\n",
        "for item in paired_data:\n",
        "    train_files_list.append({\n",
        "        \"fixed_path\": item[\"fixed_path\"],\n",
        "        \"moving_path\": item[\"moving_path\"],\n",
        "        \"fixed_label_path\": item[\"fixed_label_path\"],\n",
        "        \"moving_label_path\": item[\"moving_label_path\"]\n",
        "    })\n",
        "\n",
        "split_idx = int(0.8 * len(train_files_list))\n",
        "\n",
        "train_split = train_files_list[:split_idx]\n",
        "val_split = train_files_list[split_idx:]\n",
        "\n",
        "print(f\"ðŸ“Š Training: {len(train_split)} pairs, Validation: {len(val_split)} pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba77249",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 8: Launch Training\n",
        "\n",
        "This cell defines hyperparameters and launches training on the GPU compute pool via Snowflake ML Jobs.\n",
        "\n",
        "**Expected Training Time**: ~10-30 minutes depending on dataset size and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5b33a9e5-66f0-4657-8312-e75eaa04039d",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell8"
      },
      "outputs": [],
      "source": [
        "session.use_database(\"SF_CLINICAL_DB\")\n",
        "session.use_schema(\"UTILS\")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING CONFIGURATION & HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CT WINDOWING PARAMETERS (Hounsfield Units)\n",
        "# ============================================================================\n",
        "# These define the CT intensity range we're interested in for lung imaging\n",
        "CT_MIN_HU = -1000  # Air (lowest HU in lungs)\n",
        "CT_MAX_HU = 500    # Soft tissue/bone (upper limit for lung CT)\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL & TRAINING HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "CONFIG = {\n",
        "    # Image size after preprocessing (downsampled for memory efficiency)\n",
        "    \"spatial_size\": (96, 96, 104),  # (Height, Width, Depth)\n",
        "    \n",
        "    # Network architecture parameter (controls model capacity)\n",
        "    \"num_channel_initial\": 32,  # Number of feature channels in first layer\n",
        "    \n",
        "    # Training parameters\n",
        "    \"batch_size\": 2,            # Number of image pairs per batch (limited by GPU memory)\n",
        "    \"learning_rate\": 1e-4,      # Adam optimizer learning rate\n",
        "    \"max_epochs\": 25,           # Total training iterations through dataset\n",
        "    \n",
        "    # Loss function weighting\n",
        "    \"reg_weight\": 1.0,          # Weight for regularization (smoothness penalty)\n",
        "                                # Lower = more flexible deformations\n",
        "                                # Higher = smoother but potentially less accurate\n",
        "    \n",
        "    # Checkpointing\n",
        "    \"save_interval\": 10,        # Save model checkpoint every N epochs\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LAUNCH TRAINING AS SNOWFLAKE ML JOB\n",
        "# ============================================================================\n",
        "\n",
        "# Submit training job to GPU compute pool\n",
        "# The @remote decorator handles job submission automatically\n",
        "print(\"ðŸš€ Submitting training job to compute pool...\")\n",
        "\n",
        "training_job = train_registration_model(\n",
        "    train_files=train_split,       # Training image pairs\n",
        "    val_files=val_split,           # Validation image pairs\n",
        "    config=CONFIG,                 # Hyperparameters defined above\n",
        "    save_path=\"@SF_CLINICAL_DB.UTILS.RESULTS_STG\"  # Snowflake stage for checkpoints\n",
        ")\n",
        "\n",
        "print(f\"ðŸ“‹ Job ID: {training_job.id}\")\n",
        "print(f\"ðŸ“Š Status: {training_job.status}\")\n",
        "\n",
        "# Monitor job progress\n",
        "import time\n",
        "while training_job.status not in [\"DONE\", \"FAILED\"]:\n",
        "    print(f\"â³ Job status: {training_job.status}\")\n",
        "    time.sleep(30)  # Check every 30 seconds\n",
        "\n",
        "# Get final result\n",
        "if training_job.status == \"DONE\":\n",
        "    training_result = training_job.result()\n",
        "    st.success(f\"âœ… Training Complete! Best Dice: {training_result['best_val_dice']:.4f}\")\n",
        "    st.json(training_result)\n",
        "else:\n",
        "    print(\"âŒ Training failed. Logs:\")\n",
        "    print(training_job.get_logs())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b9263d1",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 9: Verify Saved Model Checkpoints\n",
        "\n",
        "Let's verify that our model was successfully saved to the Snowflake stage during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d8def641-cab2-411e-afea-4cddd375a70a",
      "metadata": {
        "language": "sql",
        "name": "cell12",
        "resultVariableName": "dataframe_1"
      },
      "outputs": [],
      "source": [
        "%%sql -r dataframe_1\n",
        "ls @results_stg;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17cffd8",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 10: Register Model in Snowflake Model Registry (run_batch Compatible)\n",
        "\n",
        "Now we register our trained model using a **Custom Model Class** that enables:\n",
        "- **`run_batch()` support** - Distributed batch inference on GPU compute pools\n",
        "- **Automatic dependency management** - No manual pip installs on workers\n",
        "- **Version Control** - Track different model versions over time\n",
        "- **Simplified deployment** - Just call `model.run_batch()` for inference\n",
        "\n",
        "The Custom Model wraps our MONAI LocalNet and handles:\n",
        "1. Loading images from Snowflake stages\n",
        "2. Preprocessing (CT windowing, resizing)\n",
        "3. Running inference\n",
        "4. Calculating metrics (Dice score)\n",
        "5. Saving results back to stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8516744e-8d09-4489-bacc-ece5379f4d66",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import io\n",
        "import os\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.ml.model import custom_model\n",
        "from snowflake.ml.registry import Registry\n",
        "from monai.networks.nets import LocalNet\n",
        "from monai.networks.blocks import Warp\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImage, EnsureChannelFirst, ScaleIntensityRange, Resize\n",
        ")\n",
        "\n",
        "session = get_active_session()\n",
        "\n",
        "CT_MIN_HU = -1000\n",
        "CT_MAX_HU = 500\n",
        "SPATIAL_SIZE = (96, 96, 104)\n",
        "\n",
        "\n",
        "class LungCTRegistrationModel(custom_model.CustomModel):\n",
        "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
        "        super().__init__(context)\n",
        "        self._model = None\n",
        "        self._warp_layer = None\n",
        "        self._device = None\n",
        "    \n",
        "    def _load_model(self):\n",
        "        if self._model is None:\n",
        "            from monai.networks.nets import LocalNet\n",
        "            from monai.networks.blocks import Warp\n",
        "            \n",
        "            model_path = self.context.path(\"model_weights\")\n",
        "            \n",
        "            self._model = LocalNet(\n",
        "                spatial_dims=3,\n",
        "                in_channels=2,\n",
        "                out_channels=3,\n",
        "                num_channel_initial=32,\n",
        "                extract_levels=[3],\n",
        "                out_activation=None,\n",
        "                out_kernel_initializer=\"zeros\"\n",
        "            )\n",
        "            \n",
        "            state_dict = torch.load(model_path, map_location='cpu')\n",
        "            self._model.load_state_dict(state_dict)\n",
        "            self._model.eval()\n",
        "            \n",
        "            self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            self._model = self._model.to(self._device)\n",
        "            self._warp_layer = Warp().to(self._device)\n",
        "    \n",
        "    def _get_transforms(self):\n",
        "        from monai.transforms import (\n",
        "            Compose, LoadImage, EnsureChannelFirst, ScaleIntensityRange, Resize\n",
        "        )\n",
        "        \n",
        "        preprocess_image = Compose([\n",
        "            LoadImage(image_only=True),\n",
        "            EnsureChannelFirst(),\n",
        "            ScaleIntensityRange(\n",
        "                a_min=-1000, a_max=500,\n",
        "                b_min=0.0, b_max=1.0, clip=True\n",
        "            ),\n",
        "            Resize(spatial_size=(96, 96, 104), mode=\"trilinear\"),\n",
        "        ])\n",
        "        \n",
        "        preprocess_label = Compose([\n",
        "            LoadImage(image_only=True),\n",
        "            EnsureChannelFirst(),\n",
        "            Resize(spatial_size=(96, 96, 104), mode=\"nearest\"),\n",
        "        ])\n",
        "        \n",
        "        return preprocess_image, preprocess_label\n",
        "    \n",
        "    def _preprocess_from_bytes(self, data: bytes, is_label: bool = False) -> torch.Tensor:\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".nii.gz\", delete=False) as tmp:\n",
        "            tmp.write(data)\n",
        "            tmp_path = tmp.name\n",
        "        \n",
        "        try:\n",
        "            preprocess_image, preprocess_label = self._get_transforms()\n",
        "            if is_label:\n",
        "                tensor = preprocess_label(tmp_path)\n",
        "            else:\n",
        "                tensor = preprocess_image(tmp_path)\n",
        "            return tensor.unsqueeze(0).to(self._device)\n",
        "        finally:\n",
        "            if os.path.exists(tmp_path):\n",
        "                os.unlink(tmp_path)\n",
        "    \n",
        "    @custom_model.inference_api\n",
        "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        self._load_model()\n",
        "        results = []\n",
        "        \n",
        "        for idx, row in input_df.iterrows():\n",
        "            case_id = row.get('CASE_ID', row.get('case_id', f'case_{idx}'))\n",
        "            \n",
        "            try:\n",
        "                fixed_img = self._preprocess_from_bytes(\n",
        "                    row.get('FIXED_PATH', row.get('fixed_path')), is_label=False)\n",
        "                moving_img = self._preprocess_from_bytes(\n",
        "                    row.get('MOVING_PATH', row.get('moving_path')), is_label=False)\n",
        "                fixed_lbl = self._preprocess_from_bytes(\n",
        "                    row.get('FIXED_LABEL_PATH', row.get('fixed_label_path')), is_label=True)\n",
        "                moving_lbl = self._preprocess_from_bytes(\n",
        "                    row.get('MOVING_LABEL_PATH', row.get('moving_label_path')), is_label=True)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    input_tensor = torch.cat((moving_img, fixed_img), dim=1)\n",
        "                    ddf = self._model(input_tensor)\n",
        "                    reg_label = self._warp_layer(moving_lbl, ddf)\n",
        "                    reg_image = self._warp_layer(moving_img, ddf)\n",
        "                    \n",
        "                    intersection = (reg_label * fixed_lbl).sum()\n",
        "                    dice = (2.0 * intersection + 1e-5) / (reg_label.sum() + fixed_lbl.sum() + 1e-5)\n",
        "                    \n",
        "                    ddf_magnitude = torch.sqrt((ddf ** 2).sum(dim=1)).mean()\n",
        "                \n",
        "                results.append({\n",
        "                    \"CASE_ID\": case_id,\n",
        "                    \"STATUS\": \"success\",\n",
        "                    \"DICE_SCORE\": float(dice.item()),\n",
        "                    \"DDF_MAGNITUDE\": float(ddf_magnitude.item()),\n",
        "                    \"DEVICE_USED\": str(self._device)\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    \"CASE_ID\": case_id,\n",
        "                    \"STATUS\": f\"failed: {str(e)}\",\n",
        "                    \"DICE_SCORE\": -1.0,\n",
        "                    \"DDF_MAGNITUDE\": -1.0,\n",
        "                    \"DEVICE_USED\": \"N/A\"\n",
        "                })\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def register_model_for_run_batch(\n",
        "    model_stage_path=\"@SF_CLINICAL_DB.UTILS.RESULTS_STG/best_model.pth\",\n",
        "    model_name=\"LUNG_CT_REGISTRATION\",\n",
        "    version_name=\"v_batch\"\n",
        "):\n",
        "    print(f\"Registering {model_name} {version_name} for run_batch()...\")\n",
        "    \n",
        "    raw_stream = session.file.get_stream(model_stage_path)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".pth\", delete=False) as tmp:\n",
        "        tmp.write(raw_stream.read())\n",
        "        local_weights_path = tmp.name\n",
        "    \n",
        "    try:\n",
        "        model_context = custom_model.ModelContext(\n",
        "            model_weights=local_weights_path\n",
        "        )\n",
        "        \n",
        "        my_model = LungCTRegistrationModel(model_context)\n",
        "        \n",
        "        registry = Registry(\n",
        "            session,\n",
        "            database_name=\"SF_CLINICAL_DB\",\n",
        "            schema_name=\"UTILS\"\n",
        "        )\n",
        "        \n",
        "        sample_input = pd.DataFrame({\n",
        "            \"CASE_ID\": [\"test\"],\n",
        "            \"FIXED_PATH\": [b\"dummy\"],\n",
        "            \"MOVING_PATH\": [b\"dummy\"],\n",
        "            \"FIXED_LABEL_PATH\": [b\"dummy\"],\n",
        "            \"MOVING_LABEL_PATH\": [b\"dummy\"]\n",
        "        })\n",
        "        \n",
        "        model_ref = registry.log_model(\n",
        "            model=my_model,\n",
        "            model_name=model_name,\n",
        "            version_name=version_name,\n",
        "            pip_requirements=[\"monai\", \"nibabel\", \"numpy\", \"pandas\", \"torch\"],\n",
        "            sample_input_data=sample_input,\n",
        "            options={\n",
        "                \"cuda_version\": \"12.3\"\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        print(f\"Registered: {model_ref.fully_qualified_model_name}\")\n",
        "        print(f\"Ready for run_batch() on GPU compute pools!\")\n",
        "        return model_ref\n",
        "        \n",
        "    finally:\n",
        "        if os.path.exists(local_weights_path):\n",
        "            os.unlink(local_weights_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b737b56",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 11: Execute Model Registration\n",
        "\n",
        "Run the registration function and verify the model appears in the registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df533a3f-99fa-4862-add3-478e8aafb1e0",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell10"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXECUTE MODEL REGISTRATION\n",
        "# ============================================================================\n",
        "\n",
        "# Option 1: Register for run_batch() (RECOMMENDED)\n",
        "# This enables distributed GPU inference via model.run_batch()\n",
        "model_ref = register_model_for_run_batch(\n",
        "    model_stage_path=\"@SF_CLINICAL_DB.UTILS.RESULTS_STG/best_model.pth\",\n",
        "    model_name=\"LUNG_CT_REGISTRATION\",\n",
        "    version_name=\"V_RUN_BATCH_8\"\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VERIFY REGISTRATION SUCCESS\n",
        "# ============================================================================\n",
        "\n",
        "registry = Registry(\n",
        "    session, \n",
        "    database_name=\"SF_CLINICAL_DB\", \n",
        "    schema_name=\"UTILS\"\n",
        ")\n",
        "\n",
        "all_models = registry.show_models()\n",
        "print(\"\\nðŸ“š All registered models in SF_CLINICAL_DB.UTILS:\")\n",
        "all_models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f496ed",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 12: Test Model Loading & Inference\n",
        "\n",
        "Verify we can load the registered model and run inference. This confirms the model is ready for deployment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69f3326-12ed-402a-bc9b-fd0ca46a0121",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.registry import Registry\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VERIFY REGISTRATION SUCCESS\n",
        "# ============================================================================\n",
        "\n",
        "registry = Registry(\n",
        "    session, \n",
        "    database_name=\"SF_CLINICAL_DB\", \n",
        "    schema_name=\"UTILS\"\n",
        ")\n",
        "\n",
        "all_models = registry.show_models()\n",
        "print(\"\\nðŸ“š All registered models in SF_CLINICAL_DB.UTILS:\")\n",
        "print(all_models)\n",
        "\n",
        "\n",
        "# Retrieve the registered model by name and version\n",
        "loaded = registry.get_model(\"LUNG_CT_REGISTRATION\").version(\"V_RUN_BATCH_8\")\n",
        "print(f\"\\nâœ… Successfully loaded: {loaded.model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d25ee158",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Training Complete\n",
        "\n",
        "### What We Accomplished\n",
        "1. Configured Snowflake ML Jobs for GPU training\n",
        "2. Loaded medical images from Snowflake stages\n",
        "3. Trained LocalNet model for CT registration using MONAI\n",
        "4. Saved model checkpoints to Snowflake stage\n",
        "5. Registered model in Snowflake Model Registry with `run_batch()` support\n",
        "6. Verified inference with test case\n",
        "\n",
        "### Model Location\n",
        "- **Checkpoints**: `@SF_CLINICAL_DB.UTILS.RESULTS_STG/best_model.pth`\n",
        "- **Registry**: `SF_CLINICAL_DB.UTILS.LUNG_CT_REGISTRATION`\n",
        "\n",
        "**Ready to run batch inference? Proceed to notebook 4!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "011207da-a08c-4432-8f76-40f755698490",
      "metadata": {
        "language": "python",
        "title": "Run Batch Inference"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RUN BATCH INFERENCE ON GPU COMPUTE POOL (Test with single case)\n",
        "# ============================================================================\n",
        "\n",
        "from snowflake.ml.model import JobSpec, OutputSpec, SaveMode, InputSpec, InputFormat, FileEncoding\n",
        "\n",
        "registry = Registry(session, database_name=\"SF_CLINICAL_DB\", schema_name=\"UTILS\")\n",
        "model = registry.get_model(\"LUNG_CT_REGISTRATION\")\n",
        "mv = model.version(\"V_RUN_BATCH_8\")\n",
        "\n",
        "input_data = [\n",
        "    {\n",
        "        \"CASE_ID\": \"case_001\",\n",
        "        \"FIXED_PATH\": \"@SF_CLINICAL_DB.UTILS.MONAI_MEDICAL_IMAGES_STG/scansExp/case_001_exp.nii.gz\",\n",
        "        \"MOVING_PATH\": \"@SF_CLINICAL_DB.UTILS.MONAI_MEDICAL_IMAGES_STG/scansInsp/case_001_insp.nii.gz\",\n",
        "        \"FIXED_LABEL_PATH\": \"@SF_CLINICAL_DB.UTILS.MONAI_MEDICAL_IMAGES_STG/lungMasksExp/case_001_exp.nii.gz\",\n",
        "        \"MOVING_LABEL_PATH\": \"@SF_CLINICAL_DB.UTILS.MONAI_MEDICAL_IMAGES_STG/lungMasksInsp/case_001_insp.nii.gz\"\n",
        "    }\n",
        "]\n",
        "\n",
        "input_df = session.create_dataframe(input_data)\n",
        "\n",
        "job = mv.run_batch(\n",
        "    compute_pool=TRAINING_COMPUTE_POOL,\n",
        "    X=input_df,\n",
        "    input_spec=InputSpec(\n",
        "        column_handling={\n",
        "            \"FIXED_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES},\n",
        "            \"MOVING_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES},\n",
        "            \"FIXED_LABEL_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES},\n",
        "            \"MOVING_LABEL_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES}\n",
        "        }\n",
        "    ),\n",
        "    output_spec=OutputSpec(\n",
        "        stage_location=\"@SF_CLINICAL_DB.UTILS.RESULTS_STG/inference_out/\",\n",
        "        mode=SaveMode.OVERWRITE\n",
        "    ),\n",
        "    job_spec=JobSpec(gpu_requests=\"1\")\n",
        ")\n",
        "\n",
        "print(\"Batch inference job submitted!\")\n",
        "print(f\"Job ID: {job.id}\")\n",
        "\n",
        "job.wait()\n",
        "print(\"Job completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2a6a1594-953d-42aa-8d50-8e97a176f7f0",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_2"
      },
      "outputs": [],
      "source": [
        "%%sql -r dataframe_2\n",
        "ls @SF_CLINICAL_DB.UTILS.RESULTS_STG/inference_out/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b7d73977-dff3-46ef-9a23-3b96ca47bd4c",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_3"
      },
      "outputs": [],
      "source": [
        "%%sql -r dataframe_3\n",
        "CREATE OR REPLACE FILE FORMAT sf_clinical_db.utils.parquet_ff TYPE = 'parquet';\n",
        "\n",
        "\n",
        "select $1\n",
        "from @SF_CLINICAL_DB.UTILS.results_stg/inference_out/3_f7de682d39c24e16b9002dd28c9d8be1_000000_000000-0.parquet\n",
        " (file_format => sf_clinical_db.utils.parquet_ff) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98f0146-1c69-439b-989b-1d400a0542aa",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py_spcs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    },
    "lastEditStatus": {
      "authorEmail": "carlos.guzman@snowflake.com",
      "authorId": "4124439897012",
      "authorName": "CGUZMAN",
      "lastEditTime": 1763698038769,
      "notebookId": "u24q2ex56v4septcdgf4",
      "sessionId": "a47d4778-ca0f-4e20-98b9-25cb3d3908b0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
