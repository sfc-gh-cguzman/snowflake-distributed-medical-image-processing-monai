{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5405335-9c19-4d6f-a571-7a76898c131b",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "!pip install snowflake-ml-python --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e783bba",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "# MONAI Lung CT Registration - Distributed Inference\n",
        "\n",
        "This notebook runs **distributed GPU inference** using Snowflake's `run_batch()` API.\n",
        "\n",
        "## Key Benefits of `run_batch()`\n",
        "- **Distributed Processing**: Automatic parallelization across GPU compute pools\n",
        "- **Scalability**: Process thousands of CT scans concurrently\n",
        "- **Simplicity**: No Ray/Spark setup required\n",
        "- **Cost Efficiency**: Auto-scaling compute pools with auto-suspend\n",
        "\n",
        "## Workflow\n",
        "1. Load input DataFrame with case IDs and presigned URLs\n",
        "2. Call `model_version.run_batch()` with GPU compute pool\n",
        "3. Results saved to output stage\n",
        "\n",
        "## Requirements\n",
        "- `snowflake-ml-python >= 1.8.0`\n",
        "- Model registered with `CustomModel` class (see training notebook)\n",
        "- GPU compute pool (e.g., `GPU_NV_S` instance family)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4907fc4a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 1: Initialize Snowflake Session\n",
        "\n",
        "Connect to Snowflake to access Model Registry and stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f13130c4",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.ml.registry import Registry\n",
        "from snowflake.ml.model import JobSpec, OutputSpec, SaveMode\n",
        "\n",
        "session = get_active_session()\n",
        "session.use_database('SF_CLINICAL_DB')\n",
        "session.use_schema('UTILS')\n",
        "print(\"âœ… Connected to Snowflake\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb7fd8c0",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 2: Prepare Input Data\n",
        "\n",
        "Create a Snowpark DataFrame with stage paths for inference. The `run_batch()` method will automatically download and convert these files to bytes using `InputSpec`.\n",
        "\n",
        "The DataFrame will have these columns:\n",
        "- `CASE_ID`: Unique identifier for each case\n",
        "- `FIXED_PATH`: Stage path to expiration CT scan\n",
        "- `MOVING_PATH`: Stage path to inspiration CT scan  \n",
        "- `FIXED_LABEL_PATH`: Stage path to expiration lung mask\n",
        "- `MOVING_LABEL_PATH`: Stage path to inspiration lung mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cd5db5",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def load_inference_cases(stage_location=\"@SF_CLINICAL_DB.UTILS.MONAI_MEDICAL_IMAGES_STG\", target_count=None):\n",
        "    \"\"\"\n",
        "    Load paired CT scan paths for inference.\n",
        "    If target_count is specified, duplicate cases to reach that count for performance testing.\n",
        "    \"\"\"\n",
        "    df_files = session.sql(\n",
        "        f\"LIST {stage_location} PATTERN = '.*.nii.gz'\"\n",
        "    ).select('\"name\"').to_pandas()\n",
        "    \n",
        "    df_files[\"name\"] = \"UTILS.\" + df_files[\"name\"]\n",
        "    \n",
        "    fixed_imgs = df_files[df_files['name'].str.contains(\"scans.*_exp\", regex=True)]\n",
        "    moving_imgs = df_files[df_files['name'].str.contains(\"scans.*_insp\", regex=True)]\n",
        "    fixed_masks = df_files[df_files['name'].str.contains(\"lungMasks.*_exp\", regex=True)]\n",
        "    moving_masks = df_files[df_files['name'].str.contains(\"lungMasks.*_insp\", regex=True)]\n",
        "    \n",
        "    pairs = []\n",
        "    for _, row in fixed_imgs.iterrows():\n",
        "        f_path = row['name']\n",
        "        filename = f_path.split('/')[-1]\n",
        "        case_id = filename.split('_exp')[0]\n",
        "        \n",
        "        m_path = moving_imgs[moving_imgs['name'].str.contains(f\"{case_id}_insp\")].iloc[0]['name']\n",
        "        fl_path = fixed_masks[fixed_masks['name'].str.contains(f\"{case_id}_exp\")].iloc[0]['name']\n",
        "        ml_path = moving_masks[moving_masks['name'].str.contains(f\"{case_id}_insp\")].iloc[0]['name']\n",
        "        \n",
        "        pairs.append({\n",
        "            \"CASE_ID\": case_id,\n",
        "            \"FIXED_PATH\": f\"@SF_CLINICAL_DB.{f_path}\",\n",
        "            \"MOVING_PATH\": f\"@SF_CLINICAL_DB.{m_path}\",\n",
        "            \"FIXED_LABEL_PATH\": f\"@SF_CLINICAL_DB.{fl_path}\",\n",
        "            \"MOVING_LABEL_PATH\": f\"@SF_CLINICAL_DB.{ml_path}\"\n",
        "        })\n",
        "    \n",
        "    print(f\"Found {len(pairs)} unique cases\")\n",
        "    \n",
        "    # Duplicate cases for performance testing\n",
        "    if target_count and target_count > len(pairs):\n",
        "        original_pairs = pairs.copy()\n",
        "        idx = 0\n",
        "        while len(pairs) < target_count:\n",
        "            dup = original_pairs[idx % len(original_pairs)].copy()\n",
        "            dup[\"CASE_ID\"] = f\"{dup['CASE_ID']}_dup{len(pairs)}\"  # Unique ID for duplicates\n",
        "            pairs.append(dup)\n",
        "            idx += 1\n",
        "        print(f\"Duplicated to {len(pairs)} cases for performance testing\")\n",
        "    \n",
        "    return session.create_dataframe(pd.DataFrame(pairs))\n",
        "\n",
        "# Load cases and duplicate to 1000 for performance testing\n",
        "input_df = load_inference_cases(target_count=1000)\n",
        "print(f\"\\nTotal cases for inference: {input_df.count()}\")\n",
        "input_df.limit(100).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6040a69f",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 3: Configure GPU Compute Pool\n",
        "\n",
        "The `run_batch()` method requires a GPU compute pool. Verify the pool exists and set the pool name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1ad925",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_1"
      },
      "outputs": [],
      "source": [
        "SHOW COMPUTE POOLS;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff752f6-9a06-4326-bbdc-9428bc91e18e",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "COMPUTE_POOL='GPU_ML_M_POOL'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run_batch_md",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 4: Run Distributed Batch Inference\n",
        "\n",
        "Now we use `run_batch()` to process all cases on the GPU compute pool.\n",
        "\n",
        "**Key Benefits:**\n",
        "- Snowflake automatically builds container images with dependencies\n",
        "- Ray orchestration is handled internally\n",
        "- Results are saved to the specified output stage\n",
        "- Job progress is visible in Snowsight UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_batch_code",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RUN DISTRIBUTED BATCH INFERENCE\n",
        "# ============================================================================\n",
        "\n",
        "from snowflake.ml.model import InputSpec, InputFormat, FileEncoding\n",
        "\n",
        "registry = Registry(\n",
        "    session=session,\n",
        "    database_name=\"SF_CLINICAL_DB\",\n",
        "    schema_name=\"UTILS\"\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"LUNG_CT_REGISTRATION\"\n",
        "MODEL_VERSION = \"V_RUN_BATCH_8\"\n",
        "\n",
        "mv = registry.get_model(MODEL_NAME).version(MODEL_VERSION)\n",
        "print(f\"Loaded model: {mv.fully_qualified_model_name}\")\n",
        "\n",
        "print(f\"\\nStarting batch inference on {COMPUTE_POOL}...\")\n",
        "print(f\"Cases: {input_df.count()}\")\n",
        "\n",
        "job = mv.run_batch(\n",
        "    compute_pool=COMPUTE_POOL,\n",
        "    X=input_df,\n",
        "    input_spec=InputSpec(\n",
        "        column_handling={\n",
        "            \"FIXED_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES},\n",
        "            \"MOVING_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES},\n",
        "            \"FIXED_LABEL_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES},\n",
        "            \"MOVING_LABEL_PATH\": {\"input_format\": InputFormat.FULL_STAGE_PATH, \"convert_to\": FileEncoding.RAW_BYTES}\n",
        "        }\n",
        "    ),\n",
        "    output_spec=OutputSpec(\n",
        "        stage_location=\"@SF_CLINICAL_DB.UTILS.RESULTS_STG/inference_out/\",\n",
        "        mode=SaveMode.OVERWRITE\n",
        "    ),\n",
        "    job_spec=JobSpec(gpu_requests=\"1\")\n",
        ")\n",
        "\n",
        "print(f\"\\nJob submitted! ID: {job.id}\")\n",
        "print(f\"Monitor in Snowsight: Jobs & Services\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "monitor_md",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 5: Monitor Job Progress\n",
        "\n",
        "Check the job status and wait for completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "monitor_code",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Check job status\n",
        "print(f\"Job Status: {job.status}\")\n",
        "\n",
        "job.wait()\n",
        "\n",
        "print(\"Job completed!\")\n",
        "job.show_logs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "results_md",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 6: Review Results\n",
        "\n",
        "The inference results are saved to the output stage and returned as a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595bd36d-a5e6-42a6-928f-644991c3f514",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_3"
      },
      "outputs": [],
      "source": [
        "ls @SF_CLINICAL_DB.UTILS.RESULTS_STG/inference_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c3ccd23-71cf-4ca7-bc87-68dce93f3d9e",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_4"
      },
      "outputs": [],
      "source": [
        "\n",
        "select $1 as res\n",
        "from @SF_CLINICAL_DB.UTILS.results_stg/inference_out\n",
        " (file_format => sf_clinical_db.utils.parquet_ff, PATTERN => '.*\\.parquet') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab471c7d-3d85-440c-87a6-0395b102680d",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "res_df = session.sql(r\"\"\"\n",
        "SELECT $1 AS res\n",
        "FROM @SF_CLINICAL_DB.UTILS.RESULTS_STG/inference_out\n",
        "(FILE_FORMAT => SF_CLINICAL_DB.UTILS.PARQUET_FF, PATTERN => '.*\\.parquet') \n",
        "\"\"\").to_pandas()\n",
        "\n",
        "# Parse JSON and extract key metrics\n",
        "results = [json.loads(row) for row in res_df['RES']]\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display clean summary\n",
        "print(\"ðŸ“Š Inference Results\")\n",
        "print(\"=\" * 50)\n",
        "display_df = results_df[['CASE_ID', 'STATUS', 'DICE_SCORE', 'DDF_MAGNITUDE', 'DEVICE_USED']].copy()\n",
        "display_df['DICE_SCORE'] = display_df['DICE_SCORE'].round(4)\n",
        "display_df['DDF_MAGNITUDE'] = display_df['DDF_MAGNITUDE'].round(4)\n",
        "display(display_df)\n",
        "\n",
        "# Summary statistics\n",
        "successful = results_df[results_df['STATUS'] == 'success']\n",
        "print(f\"\\nðŸ“ˆ Summary:\")\n",
        "print(f\"   Total cases: {len(results_df)}\")\n",
        "print(f\"   Successful: {len(successful)}\")\n",
        "print(f\"   Failed: {len(results_df) - len(successful)}\")\n",
        "if len(successful) > 0:\n",
        "    print(f\"   Mean Dice: {successful['DICE_SCORE'].mean():.4f}\")\n",
        "    print(f\"   Min Dice: {successful['DICE_SCORE'].min():.4f}\")\n",
        "    print(f\"   Max Dice: {successful['DICE_SCORE'].max():.4f}\")\n",
        "    print(f\"   Mean DDF Magnitude: {successful['DDF_MAGNITUDE'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "results_code",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Display inference results - clean view\n",
        "import json\n",
        "\n",
        "print(\"ðŸ“Š Inference Results:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Parse JSON results from parquet\n",
        "results_list = [json.loads(row['RES']) for _, row in res_df.iterrows()]\n",
        "results_df = pd.DataFrame(results_list)\n",
        "\n",
        "# Select only the key columns for display\n",
        "display_cols = ['CASE_ID', 'STATUS', 'DICE_SCORE', 'DDF_MAGNITUDE', 'DEVICE_USED']\n",
        "print(results_df[display_cols].to_string(index=False))\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸ“ˆ Summary Statistics:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "successful = results_df[results_df['STATUS'] == 'success']\n",
        "failed = results_df[results_df['STATUS'] != 'success']\n",
        "\n",
        "print(f\"   Total cases:    {len(results_df)}\")\n",
        "print(f\"   Successful:     {len(successful)}\")\n",
        "print(f\"   Failed:         {len(failed)}\")\n",
        "\n",
        "if len(successful) > 0:\n",
        "    print(f\"\\n   Dice Score:\")\n",
        "    print(f\"      Mean:        {successful['DICE_SCORE'].mean():.4f}\")\n",
        "    print(f\"      Std:         {successful['DICE_SCORE'].std():.4f}\")\n",
        "    print(f\"      Min:         {successful['DICE_SCORE'].min():.4f}\")\n",
        "    print(f\"      Max:         {successful['DICE_SCORE'].max():.4f}\")\n",
        "    \n",
        "    print(f\"\\n   DDF Magnitude:\")\n",
        "    print(f\"      Mean:        {successful['DDF_MAGNITUDE'].mean():.4f}\")\n",
        "    print(f\"      Min:         {successful['DDF_MAGNITUDE'].min():.4f}\")\n",
        "    print(f\"      Max:         {successful['DDF_MAGNITUDE'].max():.4f}\")\n",
        "    \n",
        "    print(f\"\\n   Device:         {successful['DEVICE_USED'].iloc[0]}\")\n",
        "\n",
        "if len(failed) > 0:\n",
        "    print(f\"\\nâš ï¸  Failed cases:\")\n",
        "    for _, row in failed.iterrows():\n",
        "        print(f\"      {row['CASE_ID']}: {row['STATUS']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save_md",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Step 7: Save Results to Table (Optional)\n",
        "\n",
        "Persist the inference results to a Snowflake table for querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save_code",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Save results to Snowflake table with parsed columns\n",
        "RESULTS_TABLE = \"SF_CLINICAL_DB.UTILS.MONAI_INFERENCE_RESULTS\"\n",
        "\n",
        "session.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {RESULTS_TABLE} AS\n",
        "SELECT \n",
        "    $1:CASE_ID::VARCHAR AS CASE_ID,\n",
        "    $1:STATUS::VARCHAR AS STATUS,\n",
        "    $1:DICE_SCORE::FLOAT AS DICE_SCORE,\n",
        "    $1:DDF_MAGNITUDE::FLOAT AS DDF_MAGNITUDE,\n",
        "    $1:DEVICE_USED::VARCHAR AS DEVICE_USED,\n",
        "    CURRENT_TIMESTAMP() AS INFERENCE_TIMESTAMP\n",
        "FROM @SF_CLINICAL_DB.UTILS.RESULTS_STG/inference_out\n",
        "(FILE_FORMAT => SF_CLINICAL_DB.UTILS.PARQUET_FF, PATTERN => '.*\\\\.parquet')\n",
        "\"\"\").collect()\n",
        "\n",
        "print(f\"âœ… Results saved to {RESULTS_TABLE}\")\n",
        "\n",
        "# Display saved results\n",
        "session.table(RESULTS_TABLE).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary_md",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## Summary\n",
        "\n",
        "ðŸŽ‰ **Inference completed using `run_batch()`!**\n",
        "\n",
        "### What We Did\n",
        "1. âœ… Loaded input DataFrame with case paths\n",
        "2. âœ… Called `model.run_batch()` on GPU compute pool\n",
        "3. âœ… Processed all cases in parallel\n",
        "4. âœ… Saved results to stage and table\n",
        "\n",
        "### Output Locations\n",
        "- **Stage**: `@SF_CLINICAL_DB.UTILS.RESULTS_STG/run_batch_output/`\n",
        "- **Table**: `SF_CLINICAL_DB.RESULTS.MONAI_INFERENCE_RESULTS`\n",
        "\n",
        "### Advantages Over Ray map_batches\n",
        "- **Simpler code**: ~50 lines vs ~400 lines\n",
        "- **No manual dependency management**: Snowflake builds container images\n",
        "- **Better monitoring**: Jobs visible in Snowsight UI\n",
        "- **Automatic cleanup**: No need to manage Ray cluster lifecycle"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "lastEditStatus": {
      "authorEmail": "carlos.guzman@snowflake.com",
      "authorId": "4124439897012",
      "authorName": "CGUZMAN",
      "lastEditTime": 1763700645401,
      "notebookId": "acurk37bgiosjvacszva",
      "sessionId": "8ccd1d54-9c31-413c-bf06-a0fbc428d742"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
